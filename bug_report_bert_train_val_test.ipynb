{
 "cells": [
  {
   "cell_type": "code",
   "id": "3278624a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:52:43.734813148Z",
     "start_time": "2026-01-06T22:52:40.420398145Z"
    }
   },
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Python exe:\", sys.executable)\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "print(\"Trainer import OK\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python exe: /mnt/data_ext4/git/icpe/icpe/bin/python\n",
      "CWD: /mnt/data_ext4/git/icpe\n",
      "numpy: 2.2.6\n",
      "pandas: 2.3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data_ext4/git/icpe/icpe/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1+cu128 cuda: True\n",
      "transformers: 4.57.3\n",
      "Trainer import OK\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "f42111a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:52:46.593605321Z",
     "start_time": "2026-01-06T22:52:46.534859672Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path.cwd() / \"../../icpe_data\"\n",
    "\n",
    "def find_first(pattern: str, root: Path):\n",
    "    hits = list(root.rglob(pattern))\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "# Core CSVs\n",
    "ALERTS_CSV = find_first(\"alerts_data.csv\", PROJECT_ROOT)\n",
    "BUGS_CSV = find_first(\"bugs_data.csv\", PROJECT_ROOT)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"ALERTS_CSV:\", ALERTS_CSV)\n",
    "print(\"BUGS_CSV  :\", BUGS_CSV)\n",
    "\n",
    "# Timeseries root folder (must be the folder that contains autoland1, autoland2, ...)\n",
    "ts_root_candidates = [p for p in PROJECT_ROOT.rglob(\"timeseries-data\") if p.is_dir()]\n",
    "assert len(ts_root_candidates) > 0, \"Could not find a folder named 'timeseries-data' under PROJECT_ROOT.\"\n",
    "TS_ROOT = ts_root_candidates[0]\n",
    "\n",
    "# Collect all TS files under TS_ROOT (all subfolders)\n",
    "ts_files = list(TS_ROOT.rglob(\"*_timeseries_data.csv\"))\n",
    "\n",
    "print(\"TS_ROOT:\", TS_ROOT)\n",
    "print(\"Timeseries files found:\", len(ts_files))\n",
    "\n",
    "# Sanity: how many files per subfolder\n",
    "folder_counts = pd.Series([p.parent.name for p in ts_files]).value_counts()\n",
    "print(\"\\nTimeseries files per subfolder:\")\n",
    "print(folder_counts)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /mnt/data_ext4/git/icpe/../../icpe_data\n",
      "ALERTS_CSV: /mnt/data_ext4/git/icpe/../../icpe_data/data/alerts_data.csv\n",
      "BUGS_CSV  : /mnt/data_ext4/git/icpe/../../icpe_data/data/bugs_data.csv\n",
      "TS_ROOT: /mnt/data_ext4/git/icpe/../../icpe_data/data/timeseries-data\n",
      "Timeseries files found: 5655\n",
      "\n",
      "Timeseries files per subfolder:\n",
      "mozilla-beta       1477\n",
      "autoland3           983\n",
      "autoland1           980\n",
      "autoland4           942\n",
      "autoland2           928\n",
      "firefox-android     342\n",
      "mozilla-central       2\n",
      "mozilla-release       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "f10bb2bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:52:49.394338372Z",
     "start_time": "2026-01-06T22:52:49.142543733Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "assert ALERTS_CSV is not None, \"alerts_data.csv not found. Set ALERTS_CSV manually.\"\n",
    "alerts = pd.read_csv(ALERTS_CSV, low_memory=False)\n",
    "\n",
    "bugs = None\n",
    "if BUGS_CSV is not None:\n",
    "    try:\n",
    "        bugs = pd.read_csv(BUGS_CSV, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(\"Could not load bugs_data.csv, continuing without it:\", repr(e))\n",
    "\n",
    "print(\"alerts shape:\", alerts.shape)\n",
    "print(\"bugs loaded:\", bugs is not None)\n",
    "\n",
    "SUMMARY_ID = \"alert_summary_id\"\n",
    "BUG_ID_COL = \"alert_summary_bug_number\"\n",
    "\n",
    "assert SUMMARY_ID in alerts.columns, f\"Missing {SUMMARY_ID} in alerts\"\n",
    "print(\"Has bug id col:\", BUG_ID_COL in alerts.columns)\n",
    "\n",
    "# quick peek\n",
    "display(alerts.head(3))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alerts shape: (17989, 62)\n",
      "bugs loaded: True\n",
      "Has bug id col: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   single_alert_new_value              single_alert_classifier  \\\n",
       "0                    9.60   mozilla-ldap/nchevobbe@mozilla.com   \n",
       "1                   10.58  mozilla-ldap/jdescottes@mozilla.com   \n",
       "2                   10.09  mozilla-ldap/jdescottes@mozilla.com   \n",
       "\n",
       "           alert_summary_prev_push_revision  alert_summary_id  \\\n",
       "0  702dc0aa87a8c3e4929277c1dc8220a7dd72418f           38845.0   \n",
       "1  25a29224e55d9a839d6fddde206208a453535ec8           39272.0   \n",
       "2  11f47f2a1cf08c1e0b5bb6374f2a7dd1f49c8cac           39287.0   \n",
       "\n",
       "   single_alert_amount_pct alert_summary_performance_tags  \\\n",
       "0                     4.71                            NaN   \n",
       "1                     9.59                            NaN   \n",
       "2                     5.54                            NaN   \n",
       "\n",
       "   single_alert_prev_value alert_summary_first_triaged  \\\n",
       "0                    10.08  2023-06-23 14:14:20.453894   \n",
       "1                     9.65  2023-08-14 08:38:03.490554   \n",
       "2                    10.68  2023-08-14 09:06:16.991824   \n",
       "\n",
       "   single_alert_series_signature_has_subtests  alert_summary_framework  ...  \\\n",
       "0                                       False                     12.0  ...   \n",
       "1                                       False                     12.0  ...   \n",
       "2                                       False                     12.0  ...   \n",
       "\n",
       "   single_alert_starred  \\\n",
       "0                 False   \n",
       "1                 False   \n",
       "2                 False   \n",
       "\n",
       "   single_alert_backfill_record_total_backfills_in_progress  \\\n",
       "0                                                NaN          \n",
       "1                                                NaN          \n",
       "2                                                NaN          \n",
       "\n",
       "  single_alert_t_value  single_alert_series_signature_suite  \\\n",
       "0                 9.06                                 damp   \n",
       "1                 9.03                                 damp   \n",
       "2                 8.01                                 damp   \n",
       "\n",
       "   single_alert_series_signature_extra_options  \\\n",
       "0              e10s, fission, stylo, webrender   \n",
       "1              e10s, fission, stylo, webrender   \n",
       "2              e10s, fission, stylo, webrender   \n",
       "\n",
       "   single_alert_backfill_record_status  single_alert_series_signature_tags  \\\n",
       "0                                  NaN                                 NaN   \n",
       "1                                  NaN                                 NaN   \n",
       "2                                  NaN                                 NaN   \n",
       "\n",
       "   alert_summary_triage_due_date       push_timestamp signature_id  \n",
       "0     2023-06-28 06:18:02.024736  2023-06-21 20:43:57      4768977  \n",
       "1     2023-08-16 01:00:27.106098  2023-08-09 13:49:06      4768977  \n",
       "2     2023-08-17 14:07:14.457937  2023-08-10 16:07:39      4768977  \n",
       "\n",
       "[3 rows x 62 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>single_alert_new_value</th>\n",
       "      <th>single_alert_classifier</th>\n",
       "      <th>alert_summary_prev_push_revision</th>\n",
       "      <th>alert_summary_id</th>\n",
       "      <th>single_alert_amount_pct</th>\n",
       "      <th>alert_summary_performance_tags</th>\n",
       "      <th>single_alert_prev_value</th>\n",
       "      <th>alert_summary_first_triaged</th>\n",
       "      <th>single_alert_series_signature_has_subtests</th>\n",
       "      <th>alert_summary_framework</th>\n",
       "      <th>...</th>\n",
       "      <th>single_alert_starred</th>\n",
       "      <th>single_alert_backfill_record_total_backfills_in_progress</th>\n",
       "      <th>single_alert_t_value</th>\n",
       "      <th>single_alert_series_signature_suite</th>\n",
       "      <th>single_alert_series_signature_extra_options</th>\n",
       "      <th>single_alert_backfill_record_status</th>\n",
       "      <th>single_alert_series_signature_tags</th>\n",
       "      <th>alert_summary_triage_due_date</th>\n",
       "      <th>push_timestamp</th>\n",
       "      <th>signature_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.60</td>\n",
       "      <td>mozilla-ldap/nchevobbe@mozilla.com</td>\n",
       "      <td>702dc0aa87a8c3e4929277c1dc8220a7dd72418f</td>\n",
       "      <td>38845.0</td>\n",
       "      <td>4.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.08</td>\n",
       "      <td>2023-06-23 14:14:20.453894</td>\n",
       "      <td>False</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.06</td>\n",
       "      <td>damp</td>\n",
       "      <td>e10s, fission, stylo, webrender</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-28 06:18:02.024736</td>\n",
       "      <td>2023-06-21 20:43:57</td>\n",
       "      <td>4768977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.58</td>\n",
       "      <td>mozilla-ldap/jdescottes@mozilla.com</td>\n",
       "      <td>25a29224e55d9a839d6fddde206208a453535ec8</td>\n",
       "      <td>39272.0</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.65</td>\n",
       "      <td>2023-08-14 08:38:03.490554</td>\n",
       "      <td>False</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.03</td>\n",
       "      <td>damp</td>\n",
       "      <td>e10s, fission, stylo, webrender</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-16 01:00:27.106098</td>\n",
       "      <td>2023-08-09 13:49:06</td>\n",
       "      <td>4768977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.09</td>\n",
       "      <td>mozilla-ldap/jdescottes@mozilla.com</td>\n",
       "      <td>11f47f2a1cf08c1e0b5bb6374f2a7dd1f49c8cac</td>\n",
       "      <td>39287.0</td>\n",
       "      <td>5.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.68</td>\n",
       "      <td>2023-08-14 09:06:16.991824</td>\n",
       "      <td>False</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.01</td>\n",
       "      <td>damp</td>\n",
       "      <td>e10s, fission, stylo, webrender</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-17 14:07:14.457937</td>\n",
       "      <td>2023-08-10 16:07:39</td>\n",
       "      <td>4768977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 62 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "bd113a50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:53:06.131037205Z",
     "start_time": "2026-01-06T22:52:51.440508288Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def first_non_null(s: pd.Series):\n",
    "    s2 = s.dropna()\n",
    "    return s2.iloc[0] if len(s2) else np.nan\n",
    "\n",
    "def mode_or_nan(s: pd.Series):\n",
    "    s2 = s.dropna()\n",
    "    if len(s2) == 0:\n",
    "        return np.nan\n",
    "    return s2.value_counts().idxmax()\n",
    "\n",
    "def nunique_non_null(s: pd.Series) -> int:\n",
    "    return int(s.dropna().nunique())\n",
    "\n",
    "def p90_non_null(x: pd.Series):\n",
    "    x2 = pd.to_numeric(x, errors=\"coerce\").dropna()\n",
    "    return float(np.nanpercentile(x2, 90)) if len(x2) else np.nan\n",
    "\n",
    "summary_cols_candidates = [\n",
    "    \"push_timestamp\",\n",
    "    \"alert_summary_creation_timestamp\",\n",
    "    \"alert_summary_repository\",\n",
    "    \"alert_summary_revision\",\n",
    "    \"alert_summary_push_id\",\n",
    "    \"alert_summary_prev_push_id\",\n",
    "    \"alert_summary_prev_push_revision\",\n",
    "    \"alert_summary_framework\",\n",
    "    \"alert_summary_issue_tracker\",\n",
    "    \"alert_summary_related_alerts\",\n",
    "    \"alert_summary_triage_due_date\",\n",
    "    \"alert_summary_notes\",\n",
    "]\n",
    "\n",
    "single_num_candidates = [\n",
    "    \"single_alert_amount_abs\",\n",
    "    \"single_alert_amount_pct\",\n",
    "    \"single_alert_prev_value\",\n",
    "    \"single_alert_new_value\",\n",
    "    \"single_alert_t_value\",\n",
    "]\n",
    "\n",
    "single_cat_candidates = [\n",
    "    \"single_alert_is_regression\",\n",
    "    \"single_alert_manually_created\",\n",
    "    \"single_alert_noise_profile\",\n",
    "    \"single_alert_backfill_record_status\",\n",
    "    \"single_alert_backfill_record_context\",\n",
    "    \"single_alert_series_signature_suite\",\n",
    "    \"single_alert_series_signature_test\",\n",
    "    \"single_alert_series_signature_machine_platform\",\n",
    "    \"single_alert_series_signature_measurement_unit\",\n",
    "    \"single_alert_series_signature_lower_is_better\",\n",
    "]\n",
    "\n",
    "available_summary_cols = [c for c in summary_cols_candidates if c in alerts.columns]\n",
    "available_single_num = [c for c in single_num_candidates if c in alerts.columns]\n",
    "available_single_cat = [c for c in single_cat_candidates if c in alerts.columns]\n",
    "\n",
    "print(\"Using summary cols:\", available_summary_cols)\n",
    "print(\"Using numeric cols:\", available_single_num)\n",
    "print(\"Using categorical cols:\", available_single_cat)\n",
    "\n",
    "agg_spec = {}\n",
    "for c in available_summary_cols:\n",
    "    agg_spec[c] = pd.NamedAgg(column=c, aggfunc=first_non_null)\n",
    "\n",
    "if \"signature_id\" in alerts.columns:\n",
    "    agg_spec[\"signature_id__nunique\"] = pd.NamedAgg(column=\"signature_id\", aggfunc=nunique_non_null)\n",
    "\n",
    "for c in available_single_num:\n",
    "    agg_spec[f\"{c}__mean\"] = pd.NamedAgg(column=c, aggfunc=\"mean\")\n",
    "    agg_spec[f\"{c}__max\"]  = pd.NamedAgg(column=c, aggfunc=\"max\")\n",
    "    agg_spec[f\"{c}__min\"]  = pd.NamedAgg(column=c, aggfunc=\"min\")\n",
    "    agg_spec[f\"{c}__std\"]  = pd.NamedAgg(column=c, aggfunc=\"std\")\n",
    "    agg_spec[f\"{c}__p90\"]  = pd.NamedAgg(column=c, aggfunc=p90_non_null)\n",
    "\n",
    "for c in available_single_cat:\n",
    "    agg_spec[f\"{c}__mode\"] = pd.NamedAgg(column=c, aggfunc=mode_or_nan)\n",
    "    agg_spec[f\"{c}__nunique\"] = pd.NamedAgg(column=c, aggfunc=nunique_non_null)\n",
    "\n",
    "agg_spec[\"n_single_alerts\"] = pd.NamedAgg(column=SUMMARY_ID, aggfunc=\"size\")\n",
    "\n",
    "g = alerts.groupby(SUMMARY_ID, dropna=False)\n",
    "summary_df = g.agg(**agg_spec).reset_index()\n",
    "\n",
    "# Label: bug_created and optional bug_id\n",
    "if BUG_ID_COL in alerts.columns:\n",
    "    bug_created = g[BUG_ID_COL].apply(lambda s: s.notna().any()).reset_index(name=\"bug_created\")\n",
    "    summary_df = summary_df.merge(bug_created, on=SUMMARY_ID, how=\"left\")\n",
    "\n",
    "    bug_id = g[BUG_ID_COL].apply(first_non_null).reset_index(name=\"bug_id\")\n",
    "    summary_df = summary_df.merge(bug_id, on=SUMMARY_ID, how=\"left\")\n",
    "else:\n",
    "    raise KeyError(f\"{BUG_ID_COL} not found. Cannot build label bug_created.\")\n",
    "\n",
    "# Derived time features\n",
    "if \"push_timestamp\" in summary_df.columns:\n",
    "    ts = pd.to_datetime(summary_df[\"push_timestamp\"], errors=\"coerce\", utc=True)\n",
    "    summary_df[\"push_dow\"] = ts.dt.dayofweek\n",
    "    summary_df[\"push_hour\"] = ts.dt.hour\n",
    "    summary_df[\"push_is_weekend\"] = ts.dt.dayofweek.isin([5, 6]).astype(\"int8\")\n",
    "\n",
    "# Notes length features\n",
    "if \"alert_summary_notes\" in summary_df.columns:\n",
    "    notes = summary_df[\"alert_summary_notes\"].fillna(\"\").astype(str)\n",
    "    summary_df[\"notes_len_chars\"] = notes.str.len()\n",
    "    summary_df[\"notes_len_words\"] = notes.str.split().str.len()\n",
    "\n",
    "print(\"summary_df:\", summary_df.shape, \"unique summaries:\", summary_df[SUMMARY_ID].nunique())\n",
    "print(\"bug_created rate:\", float(summary_df[\"bug_created\"].mean()))\n",
    "display(summary_df.head(3))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using summary cols: ['push_timestamp', 'alert_summary_creation_timestamp', 'alert_summary_repository', 'alert_summary_revision', 'alert_summary_push_id', 'alert_summary_prev_push_id', 'alert_summary_prev_push_revision', 'alert_summary_framework', 'alert_summary_issue_tracker', 'alert_summary_related_alerts', 'alert_summary_triage_due_date', 'alert_summary_notes']\n",
      "Using numeric cols: ['single_alert_amount_abs', 'single_alert_amount_pct', 'single_alert_prev_value', 'single_alert_new_value', 'single_alert_t_value']\n",
      "Using categorical cols: ['single_alert_is_regression', 'single_alert_manually_created', 'single_alert_noise_profile', 'single_alert_backfill_record_status', 'single_alert_backfill_record_context', 'single_alert_series_signature_suite', 'single_alert_series_signature_test', 'single_alert_series_signature_machine_platform', 'single_alert_series_signature_measurement_unit', 'single_alert_series_signature_lower_is_better']\n",
      "summary_df: (3912, 67) unique summaries: 3912\n",
      "bug_created rate: 0.16180981595092025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   alert_summary_id       push_timestamp alert_summary_creation_timestamp  \\\n",
       "0               1.0  2024-04-15 00:20:26       2024-04-25 02:30:37.945749   \n",
       "1               2.0  2024-04-25 01:53:27       2024-04-25 05:55:09.574000   \n",
       "2               3.0  2024-04-25 03:42:06       2024-04-25 09:26:16.110988   \n",
       "\n",
       "  alert_summary_repository                    alert_summary_revision  \\\n",
       "0                 autoland  8c457b0d7626241855c3c025a2531eb5f00b8cfc   \n",
       "1                 autoland  9355724ed37952f4672430cb3eb034abb466d3e8   \n",
       "2                 autoland  399ca207b66fa17a2b093b420be369286c774f1d   \n",
       "\n",
       "   alert_summary_push_id  alert_summary_prev_push_id  \\\n",
       "0              1408605.0                   1408562.0   \n",
       "1              1415606.0                   1415370.0   \n",
       "2              1415669.0                   1415644.0   \n",
       "\n",
       "           alert_summary_prev_push_revision  alert_summary_framework  \\\n",
       "0  25a22c2f939167a119d912b3fe61312db879d576                     13.0   \n",
       "1  ec5140ea1b1384a44d1ce6e0271fc97472399e96                      6.0   \n",
       "2  2adaf5befa1e2768e816e25d5d3266d6774bef01                      2.0   \n",
       "\n",
       "   alert_summary_issue_tracker  ...  \\\n",
       "0                          1.0  ...   \n",
       "1                          1.0  ...   \n",
       "2                          1.0  ...   \n",
       "\n",
       "  single_alert_series_signature_lower_is_better__mode  \\\n",
       "0                                               True    \n",
       "1                                               True    \n",
       "2                                               True    \n",
       "\n",
       "  single_alert_series_signature_lower_is_better__nunique n_single_alerts  \\\n",
       "0                                                  1                   1   \n",
       "1                                                  1                   1   \n",
       "2                                                  1                   1   \n",
       "\n",
       "   bug_created     bug_id  push_dow  push_hour  push_is_weekend  \\\n",
       "0        False        NaN         0          0                0   \n",
       "1        False        NaN         3          1                0   \n",
       "2         True  1894281.0         3          3                0   \n",
       "\n",
       "   notes_len_chars  notes_len_words  \n",
       "0                0                0  \n",
       "1                0                0  \n",
       "2                0                0  \n",
       "\n",
       "[3 rows x 67 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alert_summary_id</th>\n",
       "      <th>push_timestamp</th>\n",
       "      <th>alert_summary_creation_timestamp</th>\n",
       "      <th>alert_summary_repository</th>\n",
       "      <th>alert_summary_revision</th>\n",
       "      <th>alert_summary_push_id</th>\n",
       "      <th>alert_summary_prev_push_id</th>\n",
       "      <th>alert_summary_prev_push_revision</th>\n",
       "      <th>alert_summary_framework</th>\n",
       "      <th>alert_summary_issue_tracker</th>\n",
       "      <th>...</th>\n",
       "      <th>single_alert_series_signature_lower_is_better__mode</th>\n",
       "      <th>single_alert_series_signature_lower_is_better__nunique</th>\n",
       "      <th>n_single_alerts</th>\n",
       "      <th>bug_created</th>\n",
       "      <th>bug_id</th>\n",
       "      <th>push_dow</th>\n",
       "      <th>push_hour</th>\n",
       "      <th>push_is_weekend</th>\n",
       "      <th>notes_len_chars</th>\n",
       "      <th>notes_len_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-04-15 00:20:26</td>\n",
       "      <td>2024-04-25 02:30:37.945749</td>\n",
       "      <td>autoland</td>\n",
       "      <td>8c457b0d7626241855c3c025a2531eb5f00b8cfc</td>\n",
       "      <td>1408605.0</td>\n",
       "      <td>1408562.0</td>\n",
       "      <td>25a22c2f939167a119d912b3fe61312db879d576</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2024-04-25 01:53:27</td>\n",
       "      <td>2024-04-25 05:55:09.574000</td>\n",
       "      <td>autoland</td>\n",
       "      <td>9355724ed37952f4672430cb3eb034abb466d3e8</td>\n",
       "      <td>1415606.0</td>\n",
       "      <td>1415370.0</td>\n",
       "      <td>ec5140ea1b1384a44d1ce6e0271fc97472399e96</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2024-04-25 03:42:06</td>\n",
       "      <td>2024-04-25 09:26:16.110988</td>\n",
       "      <td>autoland</td>\n",
       "      <td>399ca207b66fa17a2b093b420be369286c774f1d</td>\n",
       "      <td>1415669.0</td>\n",
       "      <td>1415644.0</td>\n",
       "      <td>2adaf5befa1e2768e816e25d5d3266d6774bef01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1894281.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 67 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "d607acd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:53:08.841416610Z",
     "start_time": "2026-01-06T22:53:08.805442996Z"
    }
   },
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ts_files and TS_ROOT come from Cell 1\n",
    "assert \"TS_ROOT\" in globals(), \"TS_ROOT not found. Run Cell 1.\"\n",
    "assert \"ts_files\" in globals(), \"ts_files not found. Run Cell 1.\"\n",
    "assert len(ts_files) > 0, \"No *_timeseries_data.csv files found under TS_ROOT.\"\n",
    "\n",
    "# Build: folder -> {sig_id: path}\n",
    "ts_index = defaultdict(dict)\n",
    "\n",
    "# Build reverse map: sig_id -> [folders...]\n",
    "sig_to_folders = defaultdict(list)\n",
    "\n",
    "for p in ts_files:\n",
    "    name = p.name\n",
    "    if not name.endswith(\"_timeseries_data.csv\"):\n",
    "        continue\n",
    "    prefix = name.replace(\"_timeseries_data.csv\", \"\")\n",
    "    try:\n",
    "        sig_id = int(prefix)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    folder = p.parent.name  # autoland1, mozilla-central, etc.\n",
    "    ts_index[folder][sig_id] = p\n",
    "    sig_to_folders[sig_id].append(folder)\n",
    "\n",
    "folders = sorted(ts_index.keys())\n",
    "\n",
    "print(\"TS_ROOT:\", TS_ROOT)\n",
    "print(\"Folders indexed:\", folders)\n",
    "print(\"Files per folder:\")\n",
    "print(pd.Series({k: len(v) for k, v in ts_index.items()}).sort_values(ascending=False))\n",
    "\n",
    "n_unique_sigs = len(sig_to_folders)\n",
    "n_multi = sum(1 for sig, fl in sig_to_folders.items() if len(set(fl)) > 1)\n",
    "print(\"Unique signature_ids indexed:\", n_unique_sigs)\n",
    "print(\"signature_ids appearing in >1 folder:\", n_multi)\n",
    "\n",
    "def folder_candidates_from_row(row: pd.Series) -> list[str]:\n",
    "    repo = str(row.get(\"alert_summary_repository\", \"\")).lower()\n",
    "    fw = str(row.get(\"alert_summary_framework\", \"\")).lower()\n",
    "    hay = repo + \" \" + fw\n",
    "\n",
    "    # Priority buckets\n",
    "    candidates = []\n",
    "\n",
    "    # Direct folder name match\n",
    "    for f in folders:\n",
    "        if f.lower() in hay:\n",
    "            candidates.append(f)\n",
    "\n",
    "    # Heuristics for common cases\n",
    "    if \"autoland\" in hay:\n",
    "        candidates.extend([f for f in folders if f.startswith(\"autoland\")])\n",
    "    if \"android\" in hay:\n",
    "        candidates.extend([f for f in folders if \"android\" in f])\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for c in candidates:\n",
    "        if c not in seen:\n",
    "            out.append(c)\n",
    "            seen.add(c)\n",
    "    return out\n",
    "\n",
    "def resolve_ts_path(sig_id: int, row: pd.Series | None = None) -> Path | None:\n",
    "    sig_id = int(sig_id)\n",
    "\n",
    "    # 1) Try folders suggested by this row\n",
    "    if row is not None:\n",
    "        for f in folder_candidates_from_row(row):\n",
    "            p = ts_index.get(f, {}).get(sig_id)\n",
    "            if p is not None:\n",
    "                return p\n",
    "\n",
    "    # 2) Fallback: any folder that contains the sig_id\n",
    "    fl = sig_to_folders.get(sig_id, [])\n",
    "    for f in fl:\n",
    "        p = ts_index.get(f, {}).get(sig_id)\n",
    "        if p is not None:\n",
    "            return p\n",
    "\n",
    "    return None\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TS_ROOT: /mnt/data_ext4/git/icpe/../../icpe_data/data/timeseries-data\n",
      "Folders indexed: ['autoland1', 'autoland2', 'autoland3', 'autoland4', 'firefox-android', 'mozilla-beta', 'mozilla-central', 'mozilla-release']\n",
      "Files per folder:\n",
      "mozilla-beta       1477\n",
      "autoland3           983\n",
      "autoland1           980\n",
      "autoland4           942\n",
      "autoland2           928\n",
      "firefox-android     342\n",
      "mozilla-central       2\n",
      "mozilla-release       1\n",
      "dtype: int64\n",
      "Unique signature_ids indexed: 5655\n",
      "signature_ids appearing in >1 folder: 0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "7c683e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:53:26.753444193Z",
     "start_time": "2026-01-06T22:53:22.419819764Z"
    }
   },
   "source": [
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# TS window parameters + cache\n",
    "# -----------------------------\n",
    "N_PRE = 20\n",
    "N_POST = 10\n",
    "\n",
    "CACHE_DIR = Path(\"./derived_features\")  / \"../../icpe_data/\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_FILE = CACHE_DIR / f\"ts_features_ALLFOLDERS_pre{N_PRE}_post{N_POST}.parquet\"\n",
    "\n",
    "# -----------------------------\n",
    "# Anchor columns (what we align TS on)\n",
    "# -----------------------------\n",
    "ANCHOR_PUSH_COL = \"alert_summary_push_id\" if \"alert_summary_push_id\" in summary_df.columns else None\n",
    "ANCHOR_REV_COL  = \"alert_summary_revision\" if \"alert_summary_revision\" in summary_df.columns else None\n",
    "\n",
    "if ANCHOR_PUSH_COL is None and ANCHOR_REV_COL is None:\n",
    "    raise KeyError(\"Need alert_summary_push_id or alert_summary_revision in summary_df to anchor TS windows.\")\n",
    "\n",
    "print(\"ANCHOR_PUSH_COL:\", ANCHOR_PUSH_COL)\n",
    "print(\"ANCHOR_REV_COL :\", ANCHOR_REV_COL)\n",
    "\n",
    "# -----------------------------\n",
    "# Build summary -> signature list + join needed row context for path resolution\n",
    "# -----------------------------\n",
    "summary_sigs = (\n",
    "    alerts.groupby(SUMMARY_ID)[\"signature_id\"]\n",
    "    .apply(lambda s: sorted(set(pd.to_numeric(s, errors=\"coerce\").dropna().astype(int).tolist())))\n",
    "    .reset_index(name=\"signature_ids\")\n",
    ")\n",
    "\n",
    "needed_cols = [SUMMARY_ID, \"alert_summary_repository\", \"alert_summary_framework\"]\n",
    "if ANCHOR_PUSH_COL: needed_cols.append(ANCHOR_PUSH_COL)\n",
    "if ANCHOR_REV_COL: needed_cols.append(ANCHOR_REV_COL)\n",
    "\n",
    "needed_cols = [c for c in needed_cols if c in summary_df.columns]\n",
    "summary_sigs = summary_sigs.merge(summary_df[needed_cols], on=SUMMARY_ID, how=\"left\")\n",
    "\n",
    "print(\"summary_sigs shape:\", summary_sigs.shape)\n",
    "display(summary_sigs.head(3))\n",
    "\n",
    "# -----------------------------\n",
    "# Cached loader by file path (fast)\n",
    "# -----------------------------\n",
    "@lru_cache(maxsize=4096)\n",
    "def load_timeseries_by_path(path_str: str) -> pd.DataFrame:\n",
    "    p = Path(path_str)\n",
    "    df = pd.read_csv(p, low_memory=False)\n",
    "\n",
    "    keep = [c for c in [\"push_id\", \"revision\", \"value\"] if c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "\n",
    "    if \"push_id\" in df.columns:\n",
    "        df[\"push_id\"] = pd.to_numeric(df[\"push_id\"], errors=\"coerce\")\n",
    "    if \"value\" in df.columns:\n",
    "        df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna(subset=[\"value\"])\n",
    "    if \"push_id\" in df.columns:\n",
    "        df = df.dropna(subset=[\"push_id\"]).sort_values(\"push_id\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def safe_slope(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(np.polyfit(x, y, 1)[0])\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def compute_window_features(ts: pd.DataFrame, anchor_push_id=None, anchor_revision=None, n_pre=20, n_post=10):\n",
    "    out = {\n",
    "        \"ts_n_total\": int(len(ts)) if ts is not None else 0,\n",
    "        \"ts_n_pre\": np.nan,\n",
    "        \"ts_n_post\": np.nan,\n",
    "        \"ts_delta_mean_rel\": np.nan,\n",
    "        \"ts_delta_mean\": np.nan,\n",
    "        \"ts_z_pre_post\": np.nan,\n",
    "        \"ts_delta_std\": np.nan,\n",
    "        \"ts_slope_change\": np.nan,\n",
    "    }\n",
    "    if ts is None or ts.empty or \"value\" not in ts.columns:\n",
    "        return out\n",
    "\n",
    "    anchor_idx = None\n",
    "\n",
    "    # Prefer push_id alignment\n",
    "    if anchor_push_id is not None and \"push_id\" in ts.columns and pd.notna(anchor_push_id):\n",
    "        ap = pd.to_numeric(anchor_push_id, errors=\"coerce\")\n",
    "        if pd.notna(ap):\n",
    "            push_ids = ts[\"push_id\"].to_numpy()\n",
    "            exact = np.where(push_ids == ap)[0]\n",
    "            if len(exact):\n",
    "                anchor_idx = int(exact[0])\n",
    "            else:\n",
    "                le = np.where(push_ids <= ap)[0]\n",
    "                if len(le):\n",
    "                    anchor_idx = int(le[-1])\n",
    "\n",
    "    # Fallback to revision alignment\n",
    "    if anchor_idx is None and anchor_revision is not None and \"revision\" in ts.columns and pd.notna(anchor_revision):\n",
    "        rev = str(anchor_revision)\n",
    "        matches = np.where(ts[\"revision\"].astype(str).to_numpy() == rev)[0]\n",
    "        if len(matches):\n",
    "            anchor_idx = int(matches[0])\n",
    "\n",
    "    if anchor_idx is None:\n",
    "        return out\n",
    "\n",
    "    values = ts[\"value\"].to_numpy()\n",
    "    pre_start = max(0, anchor_idx - n_pre)\n",
    "    pre = values[pre_start:anchor_idx]\n",
    "    post_end = min(len(values), anchor_idx + 1 + n_post)\n",
    "    post = values[anchor_idx + 1:post_end]\n",
    "\n",
    "    out[\"ts_n_pre\"] = int(len(pre))\n",
    "    out[\"ts_n_post\"] = int(len(post))\n",
    "    if len(pre) == 0 or len(post) == 0:\n",
    "        return out\n",
    "\n",
    "    pre_mean = float(np.nanmean(pre))\n",
    "    post_mean = float(np.nanmean(post))\n",
    "\n",
    "    pre_std = float(np.nanstd(pre, ddof=1)) if len(pre) > 1 else np.nan\n",
    "    post_std = float(np.nanstd(post, ddof=1)) if len(post) > 1 else np.nan\n",
    "\n",
    "    denom = max(abs(pre_mean), 1e-9)\n",
    "    out[\"ts_delta_mean_rel\"] = (post_mean - pre_mean) / denom\n",
    "    out[\"ts_delta_mean\"] = post_mean - pre_mean\n",
    "    out[\"ts_delta_std\"] = (post_std - pre_std) if (pd.notna(pre_std) and pd.notna(post_std)) else np.nan\n",
    "\n",
    "    x_pre = np.arange(len(pre), dtype=float)\n",
    "    x_post = np.arange(len(post), dtype=float)\n",
    "    pre_slope = safe_slope(x_pre, pre.astype(float))\n",
    "    post_slope = safe_slope(x_post, post.astype(float))\n",
    "    if pd.notna(pre_slope) and pd.notna(post_slope):\n",
    "        out[\"ts_slope_change\"] = post_slope - pre_slope\n",
    "\n",
    "    if pd.notna(pre_std) and pre_std > 0:\n",
    "        out[\"ts_z_pre_post\"] = (post_mean - pre_mean) / pre_std\n",
    "\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# Compute TS features per summary (cached)\n",
    "# -----------------------------\n",
    "if CACHE_FILE.exists():\n",
    "    ts_feat_df = pd.read_parquet(CACHE_FILE)\n",
    "    print(\"Loaded cached TS features:\", ts_feat_df.shape)\n",
    "else:\n",
    "    rows = []\n",
    "\n",
    "    for row in summary_sigs.itertuples(index=False):\n",
    "        sid = int(getattr(row, SUMMARY_ID))\n",
    "        sigs = getattr(row, \"signature_ids\")\n",
    "\n",
    "        anchor_push = getattr(row, ANCHOR_PUSH_COL) if ANCHOR_PUSH_COL else None\n",
    "        anchor_rev = getattr(row, ANCHOR_REV_COL) if ANCHOR_REV_COL else None\n",
    "\n",
    "        # Reconstruct row as Series for resolve_ts_path\n",
    "        row_series = pd.Series(row._asdict())\n",
    "\n",
    "        per_sig_feats = []\n",
    "        missing = 0\n",
    "\n",
    "        for sig in sigs:\n",
    "            p = resolve_ts_path(int(sig), row=row_series)\n",
    "            if p is None:\n",
    "                missing += 1\n",
    "                continue\n",
    "\n",
    "            ts = load_timeseries_by_path(str(p))\n",
    "            if ts.empty:\n",
    "                missing += 1\n",
    "                continue\n",
    "\n",
    "            f = compute_window_features(\n",
    "                ts,\n",
    "                anchor_push_id=anchor_push,\n",
    "                anchor_revision=anchor_rev,\n",
    "                n_pre=N_PRE,\n",
    "                n_post=N_POST\n",
    "            )\n",
    "            per_sig_feats.append(f)\n",
    "\n",
    "        agg_row = {SUMMARY_ID: sid}\n",
    "        agg_row[\"ts_sig_used\"] = int(len(per_sig_feats))\n",
    "        agg_row[\"ts_sig_missing\"] = int(missing)\n",
    "\n",
    "        if len(per_sig_feats) == 0:\n",
    "            rows.append(agg_row)\n",
    "            continue\n",
    "\n",
    "        feats_df = pd.DataFrame(per_sig_feats)\n",
    "\n",
    "        for base in [\"ts_delta_mean_rel\", \"ts_delta_mean\", \"ts_z_pre_post\", \"ts_delta_std\", \"ts_slope_change\", \"ts_n_pre\", \"ts_n_post\"]:\n",
    "            v = pd.to_numeric(feats_df.get(base, np.nan), errors=\"coerce\")\n",
    "            agg_row[f\"{base}__mean_over_sigs\"] = float(v.mean())\n",
    "            agg_row[f\"{base}__max_over_sigs\"] = float(v.max())\n",
    "\n",
    "        rows.append(agg_row)\n",
    "\n",
    "    ts_feat_df = pd.DataFrame(rows)\n",
    "    ts_feat_df.to_parquet(CACHE_FILE, index=False)\n",
    "    print(\"Computed and cached TS features:\", ts_feat_df.shape)\n",
    "\n",
    "# Merge into summary_df_full_ts\n",
    "summary_df_full_ts = summary_df.merge(ts_feat_df, on=SUMMARY_ID, how=\"left\")\n",
    "print(\"summary_df_full_ts shape:\", summary_df_full_ts.shape)\n",
    "\n",
    "# Quick coverage sanity\n",
    "print(\"\\nTS coverage summary:\")\n",
    "print(summary_df_full_ts[[\"ts_sig_used\", \"ts_sig_missing\"]].fillna(0).describe())\n",
    "print(\"Share with ts_sig_used > 0:\", float((summary_df_full_ts[\"ts_sig_used\"].fillna(0) > 0).mean()))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANCHOR_PUSH_COL: alert_summary_push_id\n",
      "ANCHOR_REV_COL : alert_summary_revision\n",
      "summary_sigs shape: (3912, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   alert_summary_id signature_ids alert_summary_repository  \\\n",
       "0               1.0     [4969895]                 autoland   \n",
       "1               2.0     [3482387]                 autoland   \n",
       "2               3.0     [3012745]                 autoland   \n",
       "\n",
       "   alert_summary_framework  alert_summary_push_id  \\\n",
       "0                     13.0              1408605.0   \n",
       "1                      6.0              1415606.0   \n",
       "2                      2.0              1415669.0   \n",
       "\n",
       "                     alert_summary_revision  \n",
       "0  8c457b0d7626241855c3c025a2531eb5f00b8cfc  \n",
       "1  9355724ed37952f4672430cb3eb034abb466d3e8  \n",
       "2  399ca207b66fa17a2b093b420be369286c774f1d  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alert_summary_id</th>\n",
       "      <th>signature_ids</th>\n",
       "      <th>alert_summary_repository</th>\n",
       "      <th>alert_summary_framework</th>\n",
       "      <th>alert_summary_push_id</th>\n",
       "      <th>alert_summary_revision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[4969895]</td>\n",
       "      <td>autoland</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1408605.0</td>\n",
       "      <td>8c457b0d7626241855c3c025a2531eb5f00b8cfc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[3482387]</td>\n",
       "      <td>autoland</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1415606.0</td>\n",
       "      <td>9355724ed37952f4672430cb3eb034abb466d3e8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[3012745]</td>\n",
       "      <td>autoland</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1415669.0</td>\n",
       "      <td>399ca207b66fa17a2b093b420be369286c774f1d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e226ec0",
   "metadata": {},
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = summary_df_full_ts.copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Time-based split (train/val/test)\n",
    "#    - Train: oldest 70%\n",
    "#    - Val  : next   10%\n",
    "#    - Test : newest 20%   (held out; never used for model selection)\n",
    "# -----------------------------\n",
    "TEST_FRAC = 0.20\n",
    "VAL_FRAC  = 0.10\n",
    "\n",
    "time_col = None\n",
    "for c in [\"push_timestamp\", \"alert_summary_creation_timestamp\"]:\n",
    "    if c in df.columns and pd.to_datetime(df[c], errors=\"coerce\", utc=True).notna().any():\n",
    "        time_col = c\n",
    "        break\n",
    "if time_col is None:\n",
    "    raise ValueError(\"No usable time column found for time-based split.\")\n",
    "\n",
    "t = pd.to_datetime(df[time_col], errors=\"coerce\", utc=True)\n",
    "df = df.loc[t.notna()].copy()\n",
    "t = t.loc[t.notna()].copy()\n",
    "\n",
    "df[\"bug_created\"] = df[\"bug_created\"].astype(int)\n",
    "\n",
    "order = np.argsort(t.values)\n",
    "df_sorted = df.iloc[order].reset_index(drop=True)\n",
    "t_sorted = t.iloc[order].reset_index(drop=True)\n",
    "\n",
    "n = len(df_sorted)\n",
    "n_test = max(1, int(TEST_FRAC * n))\n",
    "n_val  = max(1, int(VAL_FRAC  * n))\n",
    "n_train = n - n_val - n_test\n",
    "\n",
    "# Guardrails for very small datasets\n",
    "if n_train < 1:\n",
    "    n_train = 1\n",
    "    # rebalance val/test if needed\n",
    "    remaining = n - n_train\n",
    "    n_val = max(1, int(0.33 * remaining)) if remaining > 1 else 1\n",
    "    n_test = max(1, remaining - n_val)\n",
    "\n",
    "cut_train = n_train\n",
    "cut_val = n_train + n_val\n",
    "\n",
    "train_idx = np.arange(0, cut_train)\n",
    "val_idx   = np.arange(cut_train, cut_val)\n",
    "test_idx  = np.arange(cut_val, n)\n",
    "\n",
    "print(\"Time col:\", time_col)\n",
    "print(\"Train:\", t_sorted.iloc[0], \"to\", t_sorted.iloc[cut_train-1], \"n=\", len(train_idx))\n",
    "print(\"Val  :\", t_sorted.iloc[cut_train], \"to\", t_sorted.iloc[cut_val-1], \"n=\", len(val_idx))\n",
    "print(\"Test :\", t_sorted.iloc[cut_val], \"to\", t_sorted.iloc[-1], \"n=\", len(test_idx))\n",
    "\n",
    "print(\"Train pos rate:\", float(df_sorted.loc[train_idx, \"bug_created\"].mean()))\n",
    "print(\"Val   pos rate:\", float(df_sorted.loc[val_idx, \"bug_created\"].mean()))\n",
    "print(\"Test  pos rate:\", float(df_sorted.loc[test_idx, \"bug_created\"].mean()))\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Notes sanitization\n",
    "# -----------------------------\n",
    "_bug_url_pat = re.compile(r\"https?://\\S*bugzilla\\S*\", flags=re.IGNORECASE)\n",
    "_bug_num_pat = re.compile(r\"\\bbug\\s*#?\\s*\\d+\\b\", flags=re.IGNORECASE)\n",
    "_num_pat = re.compile(r\"\\b\\d{4,}\\b\")\n",
    "\n",
    "def sanitize_notes(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = _bug_url_pat.sub(\" BUGZILLA_URL \", s)\n",
    "    s = _bug_num_pat.sub(\" BUG_ID \", s)\n",
    "    s = _num_pat.sub(\" LONG_NUM \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def fill_empty(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s).strip()\n",
    "    return s if s else \"NO_NOTES\"\n",
    "\n",
    "if \"alert_summary_notes\" not in df_sorted.columns:\n",
    "    df_sorted[\"alert_summary_notes\"] = \"\"\n",
    "\n",
    "df_sorted[\"notes_raw\"] = df_sorted[\"alert_summary_notes\"].map(fill_empty)\n",
    "df_sorted[\"notes_sanitized\"] = df_sorted[\"notes_raw\"].map(sanitize_notes).map(fill_empty)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Fused text (notes + safe metadata + TS aggregates)\n",
    "# -----------------------------\n",
    "SAFE_TEXT_FIELDS = [\n",
    "    \"alert_summary_repository\",\n",
    "    \"alert_summary_framework\",\n",
    "    \"single_alert_series_signature_suite__mode\",\n",
    "    \"single_alert_series_signature_test__mode\",\n",
    "    \"single_alert_series_signature_machine_platform__mode\",\n",
    "    \"single_alert_noise_profile__mode\",\n",
    "    \"n_single_alerts\",\n",
    "    \"single_alert_amount_pct__mean\",\n",
    "    \"single_alert_amount_pct__max\",\n",
    "    \"single_alert_t_value__max\",\n",
    "    \"ts_sig_used\",\n",
    "    \"ts_sig_missing\",\n",
    "    \"ts_delta_mean_rel__mean_over_sigs\",\n",
    "    \"ts_delta_mean_rel__max_over_sigs\",\n",
    "    \"ts_z_pre_post__max_over_sigs\",\n",
    "    \"ts_slope_change__max_over_sigs\",\n",
    "]\n",
    "\n",
    "SAFE_TEXT_FIELDS = [c for c in SAFE_TEXT_FIELDS if c in df_sorted.columns]\n",
    "\n",
    "def to_token(k: str, v) -> str:\n",
    "    if pd.isna(v):\n",
    "        return \"\"\n",
    "    if isinstance(v, (float, np.floating)):\n",
    "        return f\"{k}={float(v):.4g}\"\n",
    "    return f\"{k}={str(v)}\"\n",
    "\n",
    "def build_fused_text(row: pd.Series, base_col: str) -> str:\n",
    "    parts = [row[base_col]]\n",
    "    for c in SAFE_TEXT_FIELDS:\n",
    "        tok = to_token(c, row[c])\n",
    "        if tok:\n",
    "            parts.append(tok)\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "df_sorted[\"fused_text_raw\"] = df_sorted.apply(lambda r: build_fused_text(r, \"notes_raw\"), axis=1)\n",
    "df_sorted[\"fused_text_sanitized\"] = df_sorted.apply(lambda r: build_fused_text(r, \"notes_sanitized\"), axis=1)\n",
    "\n",
    "display(df_sorted[[\"bug_created\", \"notes_sanitized\", \"fused_text_sanitized\"]].head(3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d824ea1",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from datasets import Dataset\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Metrics helpers           \n",
    "# -----------------------------\n",
    "def precision_recall_at_k(y_true, y_score, k: int):\n",
    "    k = int(min(k, len(y_true)))\n",
    "    idx = np.argsort(-y_score)[:k]\n",
    "    y_top = np.array(y_true)[idx]\n",
    "    precision = float(y_top.mean())\n",
    "    recall = float(y_top.sum() / max(1, np.sum(y_true)))\n",
    "    return precision, recall\n",
    "\n",
    "def report_metrics(y_true: np.ndarray, y_score: np.ndarray, ks=(50, 100, 200)):\n",
    "    out = {}\n",
    "    out[\"AUPRC\"] = float(average_precision_score(y_true, y_score))\n",
    "    try:\n",
    "        out[\"AUROC\"] = float(roc_auc_score(y_true, y_score))\n",
    "    except ValueError:\n",
    "        out[\"AUROC\"] = float(\"nan\")\n",
    "    for k in ks:\n",
    "        p, r = precision_recall_at_k(y_true, y_score, k)\n",
    "        out[f\"P@{min(k, len(y_true))}\"] = p\n",
    "        out[f\"R@{min(k, len(y_true))}\"] = r\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Build HF datasets from df_sorted\n",
    "# -----------------------------\n",
    "def make_hf_datasets(text_col: str):\n",
    "    \"\"\"Create Hugging Face datasets using the global time-based indices.\n",
    "\n",
    "    Returns:\n",
    "        (train_ds, val_ds, test_ds)\n",
    "    \"\"\"\n",
    "    assert text_col in df_sorted.columns, f\"{text_col} not found in df_sorted\"\n",
    "\n",
    "    train_df = df_sorted.loc[train_idx, [text_col, \"bug_created\"]].copy()\n",
    "    val_df   = df_sorted.loc[val_idx,   [text_col, \"bug_created\"]].copy()\n",
    "    test_df  = df_sorted.loc[test_idx,  [text_col, \"bug_created\"]].copy()\n",
    "\n",
    "    train_df = train_df.rename(columns={text_col: \"text\", \"bug_created\": \"label\"})\n",
    "    val_df   = val_df.rename(columns={text_col: \"text\", \"bug_created\": \"label\"})\n",
    "    test_df  = test_df.rename(columns={text_col: \"text\", \"bug_created\": \"label\"})\n",
    "\n",
    "    # HF expects int labels\n",
    "    train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
    "    val_df[\"label\"]   = val_df[\"label\"].astype(int)\n",
    "    test_df[\"label\"]  = test_df[\"label\"].astype(int)\n",
    "\n",
    "    train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "    val_ds   = Dataset.from_pandas(val_df,   preserve_index=False)\n",
    "    test_ds  = Dataset.from_pandas(test_df,  preserve_index=False)\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "# Choose which input text you want for BERT first:\n",
    "TEXT_COL = \"notes_sanitized\"         # alternative: \"fused_text_sanitized\"\n",
    "\n",
    "train_ds, val_ds, test_ds = make_hf_datasets(TEXT_COL)\n",
    "print(\"Using TEXT_COL:\", TEXT_COL)\n",
    "print(\"Train:\", train_ds)\n",
    "print(\"Val  :\", val_ds)\n",
    "print(\"Test :\", test_ds)\n",
    "\n",
    "# Quick label balance check\n",
    "print(\"Train pos rate:\", float(np.mean(train_ds[\"label\"])))\n",
    "print(\"Val   pos rate:\", float(np.mean(val_ds[\"label\"])))\n",
    "print(\"Test  pos rate:\", float(np.mean(test_ds[\"label\"])))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46f929cd",
   "metadata": {},
   "source": [
    "# --- Helper Functions for Serialization ---\n",
    "\n",
    "def bin_val(col: str, value):\n",
    "    \"\"\"\n",
    "    Calculates the bin index for a numeric value based on pre-calculated bin_edges.\n",
    "    Returns -1 if missing or no edges found.\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return -1\n",
    "    \n",
    "    # Ensure bin_edges exists (computed in previous cells of your notebook)\n",
    "    if 'bin_edges' not in globals() or col not in bin_edges:\n",
    "        return -1\n",
    "        \n",
    "    edges = bin_edges[col]\n",
    "    # Find insertion point\n",
    "    idx = np.searchsorted(edges, float(value))\n",
    "    # Clip to valid bin range (0 to N_BINS-1)\n",
    "    # We subtract 1 because searchsorted returns the index where it *would* go\n",
    "    bin_idx = max(0, min(len(edges) - 2, int(idx) - 1))\n",
    "    return bin_idx\n",
    "\n",
    "def num_to_token(col: str, q: int) -> str:\n",
    "    \"\"\"Creates a token string like 'feature=q5'.\"\"\"\n",
    "    if q < 0:\n",
    "        return \"\" # Skip missing values\n",
    "    return f\"{col}=q{q}\"\n",
    "\n",
    "def cat_to_token(col: str, val) -> str:\n",
    "    \"\"\"Creates a token string for categorical values.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"\"\n",
    "    # Simple sanitization: lowercase and replace spaces\n",
    "    clean_val = str(val).lower().strip().replace(\" \", \"_\")\n",
    "    # Remove non-alphanumeric characters if necessary, or keep simple\n",
    "    clean_val = re.sub(r\"[^a-z0-9_\\-\\.]\", \"\", clean_val)\n",
    "    return f\"{col}={clean_val}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d9b9da9",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "assert \"df_sorted\" in globals(), \"Run Cell 6 first (df_sorted).\"\n",
    "assert \"train_idx\" in globals() and \"val_idx\" in globals() and \"test_idx\" in globals(), \"Run Cell 6 first (train_idx/val_idx/test_idx).\"\n",
    "assert \"SUMMARY_ID\" in globals(), \"Run earlier cells (SUMMARY_ID).\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Choose which columns become tokens\n",
    "# -----------------------------\n",
    "EXCLUDE_COLS = {\n",
    "    \"bug_created\", \"bug_id\",\n",
    "    SUMMARY_ID,\n",
    "    \"alert_summary_notes\",  # we use notes_sanitized instead\n",
    "    \"notes_raw\", \"notes_sanitized\",\n",
    "    \"fused_text_raw\", \"fused_text_sanitized\",\n",
    "    \"fused_text_all\", \"all_text\",\n",
    "}\n",
    "\n",
    "# Drop likely high-cardinality IDs/hashes/timestamps that explode vocab\n",
    "DROP_IF_NAME_CONTAINS = [\n",
    "    \"revision\",\n",
    "    \"push_id\",            # raw push ids\n",
    "    \"prev_push\",          # previous push ids/revs\n",
    "    \"timestamp\",          # raw timestamps\n",
    "    \"triage_due_date\",    # raw date strings\n",
    "]\n",
    "\n",
    "candidate_cols = []\n",
    "for c in df_sorted.columns:\n",
    "    if c in EXCLUDE_COLS:\n",
    "        continue\n",
    "    cl = c.lower()\n",
    "    if any(k in cl for k in DROP_IF_NAME_CONTAINS):\n",
    "        continue\n",
    "    if cl.endswith(\"_id\"):\n",
    "        continue\n",
    "    if cl.endswith(\"bug_number\"):\n",
    "        continue\n",
    "    candidate_cols.append(c)\n",
    "\n",
    "# Remove very high-cardinality object columns (heuristic)\n",
    "MAX_CAT_UNIQUE = 500\n",
    "final_cols = []\n",
    "for c in candidate_cols:\n",
    "    if df_sorted[c].dtype == \"object\":\n",
    "        nunq = df_sorted.loc[train_idx, c].nunique(dropna=True)\n",
    "        if nunq > MAX_CAT_UNIQUE:\n",
    "            continue\n",
    "    final_cols.append(c)\n",
    "\n",
    "print(\"Tokenized feature columns:\", len(final_cols))\n",
    "print(final_cols[:30], \"...\" if len(final_cols) > 30 else \"\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Fit quantile bins for numeric columns on TRAIN only (no leakage)\n",
    "# -----------------------------\n",
    "N_BINS = 10  # Q0..Q9\n",
    "\n",
    "num_cols = df_sorted[final_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in final_cols if c not in num_cols]\n",
    "\n",
    "bin_edges = {}\n",
    "for c in num_cols:\n",
    "    x = pd.to_numeric(df_sorted.loc[train_idx, c], errors=\"coerce\").dropna().to_numpy()\n",
    "    if x.size < 5:\n",
    "        continue\n",
    "    qs = np.linspace(0, 1, N_BINS + 1)\n",
    "    edges = np.unique(np.quantile(x, qs))\n",
    "    if edges.size >= 3:  # needs at least 2 intervals\n",
    "        bin_edges[c] = edges\n",
    "\n",
    "print(\"Numeric columns:\", len(num_cols), \"| with bins:\", len(bin_edges))\n",
    "print(\"Categorical columns:\", len(cat_cols))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Token formatting helpers\n",
    "# -----------------------------\n",
    "_space_pat = re.compile(r\"\\s+\")\n",
    "_bad_pat = re.compile(r\"[^A-Za-z0-9_\\-\\.]+\")  # keep it tokenizer-friendly\n",
    "\n",
    "def clean_value(v: str, max_len: int = 40) -> str:\n",
    "    v = str(v)\n",
    "    v = v.strip().lower()\n",
    "    v = _space_pat.sub(\"_\", v)\n",
    "    v = _bad_pat.sub(\"\", v)\n",
    "    if not v:\n",
    "        return \"empty\"\n",
    "    return v[:max_len]\n",
    "\n",
    "def num_to_bin_token(col: str, v) -> str:\n",
    "    if pd.isna(v):\n",
    "        return f\"{col}=missing\"\n",
    "    vv = float(v)\n",
    "    edges = bin_edges.get(col)\n",
    "    if edges is None:\n",
    "        # fallback: coarse sign token only\n",
    "        if vv > 0:\n",
    "            return f\"{col}=pos\"\n",
    "        if vv < 0:\n",
    "            return f\"{col}=neg\"\n",
    "        return f\"{col}=zero\"\n",
    "    # bucket index in 0..(len(edges)-2)\n",
    "    b = int(np.searchsorted(edges, vv, side=\"right\") - 1)\n",
    "    b = max(0, min(b, len(edges) - 2))\n",
    "    # map to Q0..Q9 style (approx)\n",
    "    q = int(round(b * (N_BINS - 1) / max(1, (len(edges) - 2))))\n",
    "    return f\"{col}=q{q}\"\n",
    "\n",
    "def cat_to_token(col: str, v) -> str:\n",
    "    if pd.isna(v):\n",
    "        return f\"{col}=missing\"\n",
    "    return f\"{col}={clean_value(v)}\"\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Build all_text (notes + tokens)\n",
    "# -----------------------------\n",
    "def build_all_text(row: pd.Series) -> str:\n",
    "    # 1. Create a natural language summary of key signals\n",
    "    signals = []\n",
    "\n",
    "    # Is it a regression? (Critical feature)\n",
    "    if row.get('single_alert_is_regression') == 1:\n",
    "        signals.append(\"This is a performance regression.\")\n",
    "\n",
    "    # Magnitude of change (Verbalize the math)\n",
    "    pct = row.get('single_alert_amount_pct', 0)\n",
    "    if pd.notna(pct) and abs(pct) > 10:\n",
    "        signals.append(f\"The performance changed significantly by {pct:.1f} percent.\")\n",
    "\n",
    "    # 2. Add the standard tokens for the rest\n",
    "    tokens = []\n",
    "    for c in num_cols: # Keep your existing binning logic\n",
    "        q = bin_val(c, row[c])\n",
    "        tokens.append(num_to_token(c, q))\n",
    "\n",
    "    # 3. Combine: [Natural Signals] + [Raw Tokens] + [User Notes]\n",
    "    base_notes = row.get(\"notes_sanitized\", \"\")\n",
    "    return \" \".join(signals) + \" | \" + \" \".join(tokens) + \" | \" + base_notes\n",
    "\n",
    "df_sorted[\"all_text\"] = df_sorted.apply(build_all_text, axis=1)\n",
    "\n",
    "print(\"Example all_text:\")\n",
    "print(df_sorted[\"all_text\"].iloc[0][:400])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7e30106",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Configuration & Checks\n",
    "# -----------------------------\n",
    "# Check dependencies from previous cells\n",
    "assert \"df_sorted\" in globals(), \"df_sorted is missing. Run previous cells.\"\n",
    "assert \"all_text\" in df_sorted.columns, \"df_sorted['all_text'] is missing.\"\n",
    "assert \"make_hf_datasets\" in globals(), \"make_hf_datasets function is missing.\"\n",
    "assert \"report_metrics\" in globals(), \"report_metrics function is missing.\"\n",
    "\n",
    "CONFIG = {\n",
    "    \"text_col\": \"all_text\",\n",
    "    \"model_name\": \"distilbert-base-uncased\", # alternatives: \"roberta-base\", \"distilbert-base-uncased\"\n",
    "    \"seed\": 42,\n",
    "    \"max_len\": 512,\n",
    "    \"epochs\": 6, \n",
    "    \"batch_size_train\": 8,\n",
    "    \"batch_size_eval\": 16,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "}\n",
    "\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Data Preparation\n",
    "# -----------------------------\n",
    "# Create Hugging Face datasets\n",
    "train_ds, val_ds, test_ds = make_hf_datasets(CONFIG[\"text_col\"])\n",
    "\n",
    "print(f\"Model: {CONFIG['model_name']} | Text Column: {CONFIG['text_col']}\")\n",
    "print(f\"Train size: {len(train_ds)} | Val size: {len(val_ds)} | Test size: {len(test_ds)}\")\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"], use_fast=True)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"], \n",
    "        truncation=True, \n",
    "        max_length=CONFIG[\"max_len\"]\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "val_tok   = val_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "test_tok  = test_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# -----------------------------\n",
    "# 2b) Metrics on the validation/test splits (optional but useful for monitoring)\n",
    "# -----------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits -> P(class=1)\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
    "    labels = np.array(labels)\n",
    "    return report_metrics(labels, probs, ks=(50, 100, 200))\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Dynamic Class Weights\n",
    "# -----------------------------\n",
    "# Calculate weights based on actual training data imbalance\n",
    "y_train = np.array(train_ds[\"label\"])\n",
    "classes = np.unique(y_train)\n",
    "cw_values = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "\n",
    "# Convert to tensor and move to correct device later\n",
    "class_weights_tensor = torch.tensor(cw_values, dtype=torch.float)\n",
    "\n",
    "print(f\"Computed Class Weights: {dict(zip(classes, cw_values))}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Custom Weighted Trainer\n",
    "# -----------------------------\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    # CHANGE HERE: Add num_items_in_batch=None to the arguments\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            weights = self.class_weights.to(model.device)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "        else:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Model Initialization & Training\n",
    "# -----------------------------\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CONFIG[\"model_name\"], \n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./bert_runs/{CONFIG['model_name'].replace('/', '-')}-v3\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size_train\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size_eval\"],\n",
    "    num_train_epochs=CONFIG[\"epochs\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_P@50\",\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,         # Validation split (used for model selection)\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights_tensor,  # Pass the calculated weights here\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Final Evaluation\n",
    "# -----------------------------\n",
    "print(\"\\n--- Final Evaluation ---\")\n",
    "pred = trainer.predict(test_tok)\n",
    "logits = pred.predictions\n",
    "\n",
    "# Get probabilities for class 1 (Bug)\n",
    "proba_pos = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
    "y_true = np.array(test_ds[\"label\"])\n",
    "\n",
    "# Report metrics\n",
    "metrics = report_metrics(y_true, proba_pos, ks=(50, 100, 200))\n",
    "\n",
    "print(f\"Results for {CONFIG['model_name']} on {CONFIG['text_col']}\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.6f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "print(\"Best checkpoint:\", trainer.state.best_model_checkpoint)\n",
    "print(\"Best metric:\", trainer.state.best_metric)\n",
    "# -----------------------------"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6890d1af",
   "metadata": {},
   "source": [
    "#Token length analysis\n",
    "import numpy as np\n",
    "\n",
    "def token_lengths(texts, tokenizer, add_special_tokens=True, step=2048):\n",
    "    lens = []\n",
    "    for i in range(0, len(texts), step):\n",
    "        batch = texts[i:i+step]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            truncation=False,  # important: do NOT truncate\n",
    "            padding=False,\n",
    "            return_attention_mask=False,\n",
    "        )\n",
    "        lens.extend([len(ids) for ids in enc[\"input_ids\"]])\n",
    "    return np.array(lens, dtype=np.int32)\n",
    "\n",
    "# Example usage: texts from your HF dataset split (train_ds) or pandas\n",
    "train_texts = list(df_sorted[\"all_text\"])\n",
    "lens = token_lengths(train_texts, tokenizer)\n",
    "\n",
    "print(\"N:\", len(lens))\n",
    "for p in [50, 75, 80, 90, 95, 97, 99]:\n",
    "    print(f\"p{p}: {np.percentile(lens, p):.0f}\")\n",
    "print(\"max:\", lens.max())\n",
    "\n",
    "candidates = [64, 128, 256, 384, 512,1024]\n",
    "for L in candidates:\n",
    "    trunc_rate = (lens > (L)).mean()\n",
    "    print(f\"L={L:3d}  trunc_rate={trunc_rate:.3%}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1fd62fdf",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import platform\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) What we will log (schema)\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class RunRecord:\n",
    "    run_id: str\n",
    "    timestamp_utc: str\n",
    "\n",
    "    # Data + split\n",
    "    model_name: str\n",
    "    text_col: str\n",
    "    seed: int\n",
    "    n_train: int\n",
    "    n_val: int\n",
    "    n_test: int\n",
    "\n",
    "    # Tokenization\n",
    "    max_length: int\n",
    "    truncation_policy: str  # e.g., \"head\", \"tail\", \"head+tail\", \"chunked\"\n",
    "\n",
    "    # Training hyperparams\n",
    "    learning_rate: float\n",
    "    warmup_ratio: float\n",
    "    weight_decay: float\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    num_train_epochs: float\n",
    "    early_stopping_patience: int\n",
    "    class_weights: str  # store as json string\n",
    "\n",
    "    # Selection\n",
    "    metric_for_best_model: str\n",
    "    best_model_checkpoint: str\n",
    "    best_metric_value: float\n",
    "    best_epoch: Optional[float]\n",
    "\n",
    "    # Validation metrics (best checkpoint)\n",
    "    val_metrics: str  # json string\n",
    "\n",
    "    # Test metrics (best checkpoint)\n",
    "    test_metrics: str  # json string\n",
    "\n",
    "    # Environment\n",
    "    device: str\n",
    "    gpu_name: str\n",
    "    transformers_version: str\n",
    "    torch_version: str\n",
    "    python_version: str\n",
    "    platform: str\n",
    "\n",
    "    # Runtime\n",
    "    wall_time_seconds: float\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Logger utility\n",
    "# ----------------------------\n",
    "class SweepLogger:\n",
    "    def __init__(self, out_dir: str, csv_name: str = \"sweep_results.csv\"):\n",
    "        self.out_dir = out_dir\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "        self.csv_path = os.path.join(self.out_dir, csv_name)\n",
    "\n",
    "        self._t0 = None\n",
    "\n",
    "        # Create CSV with header if it does not exist\n",
    "        if not os.path.exists(self.csv_path):\n",
    "            with open(self.csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=list(asdict(self._empty_record()).keys()))\n",
    "                writer.writeheader()\n",
    "\n",
    "    def _empty_record(self) -> RunRecord:\n",
    "        return RunRecord(\n",
    "            run_id=\"\",\n",
    "            timestamp_utc=\"\",\n",
    "            model_name=\"\",\n",
    "            text_col=\"\",\n",
    "            seed=0,\n",
    "            n_train=0,\n",
    "            n_val=0,\n",
    "            n_test=0,\n",
    "            max_length=0,\n",
    "            truncation_policy=\"\",\n",
    "            learning_rate=0.0,\n",
    "            warmup_ratio=0.0,\n",
    "            weight_decay=0.0,\n",
    "            per_device_train_batch_size=0,\n",
    "            per_device_eval_batch_size=0,\n",
    "            gradient_accumulation_steps=1,\n",
    "            num_train_epochs=0.0,\n",
    "            early_stopping_patience=0,\n",
    "            class_weights=\"{}\",\n",
    "            metric_for_best_model=\"\",\n",
    "            best_model_checkpoint=\"\",\n",
    "            best_metric_value=float(\"nan\"),\n",
    "            best_epoch=None,\n",
    "            val_metrics=\"{}\",\n",
    "            test_metrics=\"{}\",\n",
    "            device=\"\",\n",
    "            gpu_name=\"\",\n",
    "            transformers_version=\"\",\n",
    "            torch_version=\"\",\n",
    "            python_version=\"\",\n",
    "            platform=\"\",\n",
    "            wall_time_seconds=0.0,\n",
    "        )\n",
    "\n",
    "    def start_timer(self):\n",
    "        self._t0 = time.time()\n",
    "\n",
    "    def stop_timer(self) -> float:\n",
    "        if self._t0 is None:\n",
    "            return 0.0\n",
    "        return time.time() - self._t0\n",
    "\n",
    "    def append(self, record: RunRecord):\n",
    "        # Write JSON sidecar too (useful for full details)\n",
    "        json_path = os.path.join(self.out_dir, f\"{record.run_id}.json\")\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as jf:\n",
    "            json.dump(asdict(record), jf, indent=2)\n",
    "\n",
    "        # Append to CSV\n",
    "        with open(self.csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=list(asdict(self._empty_record()).keys()))\n",
    "            writer.writerow(asdict(record))\n",
    "\n",
    "        return json_path\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Helper to extract metrics\n",
    "# ----------------------------\n",
    "def _jsonify_metrics(metrics: Dict[str, Any]) -> str:\n",
    "    # Trainer returns keys like eval_loss, eval_p_at_50, etc.\n",
    "    # Convert non-serializable values safely.\n",
    "    clean = {}\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, (int, float, str, bool)) or v is None:\n",
    "            clean[k] = v\n",
    "        else:\n",
    "            try:\n",
    "                clean[k] = float(v)\n",
    "            except Exception:\n",
    "                clean[k] = str(v)\n",
    "    return json.dumps(clean, sort_keys=True)\n",
    "\n",
    "\n",
    "def _get_env_info(transformers) -> Dict[str, str]:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"\"\n",
    "    return {\n",
    "        \"device\": device,\n",
    "        \"gpu_name\": gpu_name,\n",
    "        \"transformers_version\": getattr(transformers, \"__version__\", \"\"),\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"platform\": platform.platform(),\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Main function to log a run\n",
    "# ----------------------------\n",
    "def log_run_results(\n",
    "    *,\n",
    "    logger: SweepLogger,\n",
    "    transformers_module,\n",
    "    run_id: str,\n",
    "    model_name: str,\n",
    "    text_col: str,\n",
    "    seed: int,\n",
    "    train_tok,\n",
    "    val_tok,\n",
    "    test_tok,\n",
    "    max_length: int,\n",
    "    truncation_policy: str,\n",
    "    training_args,  # your TrainingArguments object\n",
    "    class_weights: Dict[int, float],\n",
    "    early_stopping_patience: int,\n",
    "    trainer,  # your (Weighted)Trainer AFTER trainer.train()\n",
    ") -> RunRecord:\n",
    "    wall = logger.stop_timer()\n",
    "\n",
    "    # Evaluate with the final loaded checkpoint (should be best if load_best_model_at_end=True)\n",
    "    val_metrics = trainer.evaluate(val_tok)\n",
    "    test_metrics = trainer.evaluate(test_tok)\n",
    "\n",
    "    # Best checkpoint info\n",
    "    best_ckpt = getattr(trainer.state, \"best_model_checkpoint\", \"\") or \"\"\n",
    "    best_metric = getattr(trainer.state, \"best_metric\", float(\"nan\"))\n",
    "    best_epoch = getattr(trainer.state, \"epoch\", None)\n",
    "\n",
    "    env = _get_env_info(transformers_module)\n",
    "\n",
    "    record = RunRecord(\n",
    "        run_id=run_id,\n",
    "        timestamp_utc=time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "\n",
    "        model_name=model_name,\n",
    "        text_col=text_col,\n",
    "        seed=int(seed),\n",
    "        n_train=len(train_tok),\n",
    "        n_val=len(val_tok),\n",
    "        n_test=len(test_tok),\n",
    "\n",
    "        max_length=int(max_length),\n",
    "        truncation_policy=str(truncation_policy),\n",
    "\n",
    "        learning_rate=float(getattr(training_args, \"learning_rate\", 0.0)),\n",
    "        warmup_ratio=float(getattr(training_args, \"warmup_ratio\", 0.0)),\n",
    "        weight_decay=float(getattr(training_args, \"weight_decay\", 0.0)),\n",
    "        per_device_train_batch_size=int(getattr(training_args, \"per_device_train_batch_size\", 0)),\n",
    "        per_device_eval_batch_size=int(getattr(training_args, \"per_device_eval_batch_size\", 0)),\n",
    "        gradient_accumulation_steps=int(getattr(training_args, \"gradient_accumulation_steps\", 1)),\n",
    "        num_train_epochs=float(getattr(training_args, \"num_train_epochs\", 0.0)),\n",
    "        early_stopping_patience=int(early_stopping_patience),\n",
    "        class_weights=json.dumps(class_weights, sort_keys=True),\n",
    "\n",
    "        metric_for_best_model=str(getattr(training_args, \"metric_for_best_model\", \"\")),\n",
    "        best_model_checkpoint=best_ckpt,\n",
    "        best_metric_value=float(best_metric) if best_metric is not None else float(\"nan\"),\n",
    "        best_epoch=float(best_epoch) if best_epoch is not None else None,\n",
    "\n",
    "        val_metrics=_jsonify_metrics(val_metrics),\n",
    "        test_metrics=_jsonify_metrics(test_metrics),\n",
    "\n",
    "        device=env[\"device\"],\n",
    "        gpu_name=env[\"gpu_name\"],\n",
    "        transformers_version=env[\"transformers_version\"],\n",
    "        torch_version=env[\"torch_version\"],\n",
    "        python_version=env[\"python_version\"],\n",
    "        platform=env[\"platform\"],\n",
    "\n",
    "        wall_time_seconds=float(wall),\n",
    "    )\n",
    "\n",
    "    json_path = logger.append(record)\n",
    "    print(f\"Logged run to CSV: {logger.csv_path}\")\n",
    "    print(f\"Logged run JSON:  {json_path}\")\n",
    "    return record\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Example usage pattern\n",
    "# ----------------------------\n",
    "\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "import transformers\n",
    "\n",
    "logger = SweepLogger(out_dir=\"runs_logs\")\n",
    "logger.start_timer()\n",
    "\n",
    "# ... build tokenizer/model/datasets ...\n",
    "# ... create training_args ...\n",
    "# ... create trainer ...\n",
    "# trainer.train()\n",
    "\n",
    "record = log_run_results(\n",
    "    logger=logger,\n",
    "    transformers_module=transformers,\n",
    "    run_id=\"distilbert_lr2e-5_wu0.1_seed42\",\n",
    "    model_name=\"distilbert-base-uncased\",\n",
    "    text_col=\"all_text\",\n",
    "    seed=42,\n",
    "    train_tok=train_tok,\n",
    "    val_tok=val_tok,\n",
    "    test_tok=test_tok,\n",
    "    max_length=512,\n",
    "    truncation_policy=\"head\",\n",
    "    training_args=training_args,\n",
    "    class_weights={0: 0.6046, 1: 2.8892},\n",
    "    early_stopping_patience=2,\n",
    "    trainer=trainer,\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57961879",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Post-training evaluation (using the best checkpoint selected on validation loss)\n",
    "# -----------------------------\n",
    "assert \"trainer\" in globals(), \"Run training cell first (Trainer not found).\"\n",
    "assert \"val_tok\" in globals() and \"test_tok\" in globals(), \"Run tokenization cell first (val_tok/test_tok not found).\"\n",
    "\n",
    "print(\"\\n--- Evaluate on validation split ---\")\n",
    "val_results = trainer.evaluate(eval_dataset=val_tok)\n",
    "print(val_results)\n",
    "\n",
    "print(\"\\n--- Evaluate on held-out test split ---\")\n",
    "test_results = trainer.evaluate(eval_dataset=test_tok)\n",
    "print(test_results)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
