# ğŸš€ Performance Regression Detection Benchmark (ICPE)

This project implements a **Machine Learning pipeline** designed to automate the triage of software performance alerts (Performance Regression Detection).

It predicts whether a performance alert triggered by CI/CD systems represents a **real software bug** or **noise** (false positive). The system operates in a strict **"Realistic Mode,"** meaning all "future" data (human notes, triage tags, manual classifications) is rigorously stripped to prevent Data Leakage, ensuring the benchmark reflects real-time decision-making capabilities.

## ğŸ§  Core Architecture

The model utilizes a Hybrid Architecture combining three distinct signal sources:

1.  **Multi-Scale Time Series Analysis:** Parallel extraction of statistical features (slope, z-score, step-change) over multiple window sizes (Short/Medium/Long) using `multiprocessing`.
2.  **Contextual NLP Embeddings:** Vectorization of technical context (Repository + Framework + Test Suite names) using **FastText** (compressed) to capture semantic relationships between test suites.
3.  **Metadata & Heuristics:** Processing of platform info, architecture (ARM/x86), and historical backfill data.

The classifier is built on **CatBoost**, optimized via **Optuna**.

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ benchmark.py           # ğŸ Main Entry Point: Runs the full pipeline (Load -> Train -> Eval -> Report)
â”œâ”€â”€ cat_boost_best.py      # ğŸ§ª Optimization Script: Hyperparameter tuning via Optuna
â”œâ”€â”€ config.py              # âš™ï¸ Configuration: Path definitions and constants
â”œâ”€â”€ data_loader.py         # ğŸ“¥ Data Ingestion: Loads raw CSVs and aggregates alerts
â”œâ”€â”€ model_utils.py         # ğŸ› ï¸ Utilities: PCA, Data Leakage prevention, CatBoost Pool creation
â”œâ”€â”€ preprocessing.py       # ğŸ§¹ Feature Engineering: NLP (FastText), Complex String/JSON parsing
â”œâ”€â”€ timeseries_multi.py    # ğŸ“ˆ Time Series Engine: Parallel extraction of history signals
â””â”€â”€ benchmark_results/     # ğŸ“Š Output: Scientific plots, error analysis logs (Auto-generated)
```

## ğŸ› ï¸ Installation & Requirements
Prerequisites
Python 3.9+

Internet access (required to download the compressed FastText model on the first run).

~16GB RAM recommended (for processing Time Series in parallel).

Dependencies
Installs the required packages:

``` text 
pip install pandas numpy matplotlib seaborn scikit-learn catboost cleanlab optuna psutil compress-fasttext tqdm pyarrow
```

## ğŸ’¾ Data Layout
The project expects a specific directory structure relative to the code location. By default, it looks for a folder named icpe_data located two levels up from the script (see config.py).

Expected Hierarchy:

``` text 
/icpe_data/                <-- Root Data Directory
    â”œâ”€â”€ alerts_data.csv    # Raw alerts export
    â”œâ”€â”€ bugs_data.csv      # Bug tracker export (Labels)
    â””â”€â”€ timeseries-data/   # Directory containing history CSVs per signature
          â”œâ”€â”€ repo_name/
          â”‚    â””â”€â”€ 123456_timeseries_data.csv
          â””â”€â”€ ...
```

## ğŸš€ Usage
1. Run the Standard Benchmark
To run the full training, evaluation, and reporting pipeline:

```text
python benchmark.py
```

What this does:

Loading: Loads and aggregates alert data.

TS Extraction: Extracts Time Series features (cached in ./derived_features as parquet).

NLP: Generates embeddings for test contexts using FastText.

Training: Trains the CatBoost model.

Evaluation: Calculates AUPRC, Precision@K.

Analysis: Runs Cleanlab to detect potential labeling errors in the ground truth.

Reporting: Exports scientific graphs and an error analysis CSV to benchmark_results/.

2. Hyperparameter Optimization
To re-optimize the CatBoost parameters using Optuna:

``` text 
python cat_boost_best.py
```

Note: This utilizes SQLite for storage and runs 500 trials with Repeated Stratified K-Fold validation.

## ğŸ“Š Outputs & Reporting
The benchmark.py script automatically generates a benchmark_results/ folder containing:

1. **scientific_feature_importance.png:** Grouped importance of Signals (TS) vs. Context (NLP) vs. Metadata.

2. **scientific_pr_curve.png:** Precision-Recall curve.

3. **scientific_latency_breakdown.png:** Waterfall chart of pipeline latency per commit (for production feasibility analysis).

4. **benchmark_errors.csv:** A detailed audit file containing the "Top Misses" (High confidence False Positives/Negatives) for manual review.


